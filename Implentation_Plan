Full Disclosure: I haven't worked extensively with any autograding functionality or servers yet in my career (in higher-ed)
but I am willing to take some time to learn more (already doing so: found out about Web-CAT etc.) about how they work and how we can implement them
correctly in this instance. However below are details of my implementation plan.

All repos where students would submit their code to will have individual .gitlab-ci.yml files and pre-written Dockerfiles
all in place. If you look at my .gitlab-ci.yml file, you will see that I added sections for three tests,
these would cover the three test cases stated in the exercise document.

I think we can set up both non student (open and closed) tests to run in a separate custom Docker images, and spin up those services for testing
but I'll leave that in the implementation plan.

The closed test could be a test suite implemented with assertions to match or cover expected results of what the
students code should do. Here perhaps we could add some autograding functions specifying student grades for several expected
outcomes.

For the student test, I added a sample testcase (see testingidentityfunc.py) to make sure my id function returns
the expected output.

When a student pushes code (an example code is the identityfunc.py python file included : which is just a python
function that returns a tuple containing arguments passed into the function) to the repo, with our pipeline already
setup, the docker image will be built and all tests cases and our autograding functionality declared as stages will kick
in, thereafter if there is any deployment that needs to occur, the deploy stage will take care of that. I have added
comments to the ci yml file to further explain what some configurations mean/do as well as an optional sast stage: see
(https://docs.gitlab.com/ee/user/project/merge_requests/sast.html) for more about SAST.

For repos that have an active deploy stage, we could setup pre-commit hooks to ensure credible commits before reaching
our build pipeline. We could also have tests to detect and ensure that committed code contain no secrets of any sorts.

Afterthought:
I should also say that I did some research online to see if any libraries out there are useful for autograding and came
across https://github.com/autolab from the folks at CMU but I am not sure if anyone at CU has looked at that yet.
There's also Gradescope but I am still reading about that.

Other (Crazy) Idea:
We could use Kubernetes to manage the containers built from the student repos and somehow integrate the teachers closed
test to test each artifact, I might be overoptimistic here.
